# AI-Generated Text Detector

### Demo Website: https://huggingface.co/spaces/Timxjl/seqxgpt_demo
### Slides: https://docs.google.com/presentation/d/1OMJSmQSXElmc571ldWF_6uBhB_p431oj2tTb4TOt7V0/edit?usp=sharing
### Dataset: https://huggingface.co/datasets/Roxanne-WANG/AI-Text_Detection
### DL Model Checkpoint: https://huggingface.co/Roxanne-WANG/AI-Text-Detection

## I. Introduction

In the era of large language models, distinguishing between human-written and AI-generated text has become increasingly critical. This repository presents the AI-Text-Detector, a comprehensive framework designed to address the challenges of AI-generated text detection at both the sentence and document levels. Our approach leverages a diverse set of datasets—including texts generated by GPT-2, GPT-3 (revision), LLaMA, and human sources—to provide a robust benchmarking environment.

- **Naive Approach (Mean Loss)**: A baseline strategy that computes the mean log-likelihood loss to differentiate between text sources.
- **Non-Deep Learning Method (Random Forest)**: A method that transforms text into feature vectors via Bag-of-Words and TF-IDF before classification.
- **Deep Learning Model (SeqXGPT)**: An advanced model that refines feature extraction through innovative operations such as patch-based averaging, 2D convolution, and patch shuffling, significantly improving detection performance for both bi-classification (AI vs. human) and multi-classification tasks.

Our goal is to provide researchers and practitioners with a reproducible and scalable solution to reliably detect AI-generated text, fostering transparency and trust in various applications—from academic integrity to media verification.

## II. Datasets

Each dataset contains six files. Within each dataset folder, based on the source of AI-generated sentences in the document, they are organized into different files. You can refer to the requirements of different tasks in the paper to arrange and merge the files. Below are SeqXGPT-Bench and two important evaluation datasets. (This folder should be put under dataset/) The SeqXGPT-Bench is a sentence-level AI-generated text (AIGT) detection dataset used for the study of fine-grained AIGT detection.

We upload raw data and data with extracted features on the [HuggingFace](https://huggingface.co/datasets/Roxanne-WANG/AI-Text_Detection).

##### Raw Data Format

```python
{
    "text": "Media playback is unsupported on your device 21 June 2013 Last updated at 12:31 BST The Market Hall Cinema in Brynmawr used to be run by the local council but when it announced its funding would stop last month, work began to find a way to keep it going. Thanks to the efforts of a group of local volunteers, the cinema has been saved and reopened under a new community initiative. The group, called \"Brynmawr Foundation\", raised enough funds to take over the lease of the building and purchase new equipment. They plan to show a mix of classic and new films, as well as host events and live performances. The Market Hall Cinema has been an important part of the town's history since it first opened in 1894, and this new initiative ensures that it will continue to be a valuable resource for the community.", 
    "prompt_len": 254, 
    "label": "gpt3re"
}
```

​	**`text`**: The full document containing both human-written and AI-generated content.

​	**`prompt_len`**: An integer marks the boundary between the sentences generated by humans and those generated by AI. The first `prompt_len` characters of the input `text`, i.e., *text[:prompt\_len]*, are the sentences generated by humans, while the rest are generated by a particular language model.

​	**`label`**: The label for each sentence, and there are six types of labels in total: `gpt2`, `llama`, `gpt3re`, `human`.

#### Processed Data Format

```python
{
    "losses": [3.0821034908294678],
    "begin_idx_list": [1],
    "ll_tokens_list": [[0.0, 4.58919095993042, 4.58919095993042, ... , 3.2181057929992676]],
    "label_int": 3, 
    "label": "gpt3re",
    "text": "Media playback is unsupported on your device 21 June 2013 Last updated at 12:31 BST The Market Hall Cinema in Brynmawr used to be run by the local council but when it announced its funding would stop last month, work began to find a way to keep it going. Thanks to the efforts of a group of local volunteers, the cinema has been saved and reopened under a new community initiative. The group, called \"Brynmawr Foundation\", raised enough funds to take over the lease of the building and purchase new equipment. They plan to show a mix of classic and new films, as well as host events and live performances. The Market Hall Cinema has been an important part of the town's history since it first opened in 1894, and this new initiative ensures that it will continue to be a valuable resource for the community.", 
}
```

**`losses`**: A list of log-likelihood loss values indicating how well the model predicts the text.

**`begin_idx_list`**: A list marking the index where AI-generated content begins in the text.

**`ll_tokens_list`**: A list of lists containing log-likelihood scores for each token in the text.

**`label_int`**: An integer representing the categorical label of the text source.

**`label`**: A string indicating the origin of the text, such as `gpt2`, `llama`, `gpt3re`, `human`.

**`text`**: The full document containing both human-written and AI-generated content.


| label_int | label  | count |
|-----------|--------|-------|
| 0         | gpt2   | 6000  |
| 1         | llama  | 5904  |
| 2         | human  | 5997  |
| 3         | gpt3re | 5994  |

#### Document-Level Detection Dataset

A dataset used to evaluate the performance of various methods in document-level AIGT detection.

```python
{
    "text": "in this paper we consider the possible existence of unstable axisymmetric modes in kerr space times , resulting from exponentially growing solutions of the teukolsky equation .  we describe a transformation that casts the radial equation that results upon separation of variables in the teukolsky equation , in the form of a schrdinger equation , and combine the properties of the solutions of this equations with some recent results on the asymptotic behaviour of spin weighted spheroidal harmonics to prove the existence of an infinite family of unstable modes .  thus we prove that the stationary region beyond a kerr black hole inner horizon is unstable under gravitational linear perturbations .  we also prove that kerr space - time with angular momentum larger than its square mass , which has a naked singularity , is unstable .", 
    "label": "human"
}
```

​	**`text`** refers to an entire document.

​	**`label`** is the label for the document, and there are six types of labels in total: `gpt2`, `llama`, `gpt3re`, `human`.

## III. Naive Approach - Mean Loss

### Method
Step 1: Calculate the mean loss for each label in training data

Step 2: Predict the label whose mean loss is closest to the test example's loss for each test example 

### Result
Overall Accuracy: 41.93% 

- Classification Metrics
  
    | Label  | Precision | Recall | F1-Score | Support |
    |--------|-----------|--------|----------|---------|
    | gpt2   | 0.3292    | 0.3206 | 0.3248   | 1232.0  |
    | llama  | 0.2337    | 0.3833 | 0.2904   | 720.0   |
    | human  | 0.4629    | 0.4520 | 0.4574   | 1228.0  |
    | gpt3re | 0.6489    | 0.4866 | 0.5561   | 1599.0  |


- Confusion Matrix

    |         | gpt2 | llama | human | gpt3re |
    |---------|------|-------|-------|--------|
    | gpt2    | 395  | 292   | 265   | 280    |
    | llama   | 134  | 276   | 214   | 96     |
    | human   | 142  | 486   | 555   | 45     |
    | gpt3re  | 529  | 127   | 165   | 778    |

## IV. Non-Deep Learning Method: Random Forest

![maxresdefault](https://github.com/user-attachments/assets/6456882c-bd0e-49ea-b99a-d9e208e4e8b1)
*image from Kaggle

### Step 1
Data is converted into a BoW via a Count Vectorizer 

### Step 2
The vectors are then transformed via a TF-IDF transformation

### Step 3
The transformations are then used to train and fit a Random Forest Classification Model

### Overall Metrics

| Metric      | Value   |
|-------------|---------|
| Accuracy    | 48.8%   |
| Precision   | 47.6%   |
| Recall      | 48.86%  |
| F1 Score    | 46.45%  |

### Per Class Metrics 

| Metric     | GPT 2   | GPT 3   | Human   | Llama   |
|------------|---------|---------|---------|---------|
| Accuracy   | 46.29%  | 89.55%  | 24.03%  | 35.59%  |
| Precision  | 39.02%  | 63.19%  | 49.75%  | 38.46%  |
| Recall     | 46.29%  | 89.55%  | 24.03%  | 35.59%  |
| F1 Score   | 42.35%  | 74.09%  | 32.4%   | 36.97%  |

### Confusion Matrix
|           | GPT 2 | GPT 3 | Human | Llama |
|-----------|--------|--------|--------|--------|
| **GPT 2** |  556   |  221   |  123   |  301   |
| **GPT 3** |   66   | 1071   |   31   |   28   |
| **Human** |  385   |  224   |  296   |  327   |
| **Llama** |  418   |  179   |  145   |  410   |


## V. Deep Learning Model -- SeqXGPT

Our model based on SeqXGPT, please refer to the [SeqXGPT](https://github.com/Jihuai-wpy/SeqXGPT/tree/main).

Our training checkpoints are saved on the [HuggingFace](https://huggingface.co/Roxanne-WANG/AI-Text-Detection).


### Inference Server

We GPT2-xl (1.5B) to construct the original features of our SeqXGPT and the contrastive features for Sniffer.

You can launch the inference server through `backend_api.py`. The startup command is as follows:

```bash
# --model: [gpt2]
python backend_api.py --port 20098 --timeout 30000 --debug --model=gpt2 --gpu=0
```

### Feature Extraction

#### Training Data

After successfully starting the related inference server, you can extract **the original features of SeqXGPT** using `gen_features.py`:

```bash
python dataset/gen_features.py --get_en_features --input_file dataset/SeqXGPT_raw/en_gpt2_lines.jsonl --output_file dataset/SeqXGPT_output/en_gpt2_lines.jsonl
```

#### Testing Data

As for test dataset which does not contain `label`, use below argument to extract features:

```bash
python dataset/gen_features.py --get_unlabeled_features --input_file dataset/SeqXGPT_output/inference.jsonl --output_file dataset/SeqXGPT_output/inference_output.jsonl
```

### Model -- SeqXGPT

We have provided the **complete code** for the model, dataloader, and train/test-related function under the `SeqXGPT` folder.

Before training and testing, please refer to the [Feature Extraction](#feature-extraction) section to extract relevant features, which will serve as the `--data_path`. The reference command is as follows:

```bash
# split train / test dataset and then train. You can adjust the train/test ratio using '--train_ratio'.
python SeqXGPT/train.py --split_dataset --data_path dataset/SeqXGPT_output --train_path dataset/SeqXGPT_output/en_gpt2_tr.jsonl --test_path dataset/SeqXGPT_output/en_gpt2_te.jsonl --gpu=0
```

```bash
# train
python SeqXGPT/train.py --gpu=0
```

```bash
# test
python SeqXGPT/train.py --gpu=0 --do_test
```

```bash
# test document-level AIGT detection
python SeqXGPT/train.py --gpu=0 --do_test --test_content
```

In our modified version of SeqXGPT, we introduce three extra operations on the document-level wave (i.e., the sequence of log probabilities) to further refine feature extraction and enhance generalization:

#### I. Patch-based Averaging:
We divide the wave into small patches and compute the average within each patch. This operation acts as a smoothing mechanism that reduces local noise and variability. By aggregating local statistics, each patch more reliably represents the dominant trend in that segment of the wave, thereby making the subsequent feature extraction more robust.

#### II. 2D Convolution Processing:
Instead of relying solely on 1D convolution (or other sequential models), we apply 2D convolution on the wave. This approach allows the model to capture local patterns along two dimensions simultaneously—both across the temporal axis and across feature channels. The 2D convolution effectively extracts rich spatial correlations and inter-channel interactions, which enhances the model’s ability to discern subtle differences in the wave patterns.

#### III. Patch Shuffling:
After dividing the wave into patches, we shuffle the order of these patches. Shuffling serves as a data augmentation strategy that encourages the model to learn features that are invariant to the exact ordering of local segments. By reducing dependency on sequential order, the model becomes less prone to overfitting and gains improved generalization performance when faced with varied or perturbed input sequences.

### Experiment Results

We evaluated our model on two tasks:

- **Bi-classification:** The goal was to identify whether a text is human-written or AI-generated.
- **Multi-classification:** The task involved classifying texts into one of four categories: `human`, `GPT2`, `GPT3-re`, or `LLaMA`.

To better understand the contributions of our proposed modifications, we compared several variants of our model:

- **SeqXGPT:** The baseline model.
- **SeqXGPT (PA):** Incorporates patch-based averaging to smooth the log-probability wave, reducing local noise.
- **SeqXGPT (Conv):** Applies 2D convolution to extract local spatial correlations across the wave, enhancing feature robustness.
- **SeqXGPT (PS):** Uses patch shuffling as a data augmentation strategy, promoting invariance to the ordering of local segments.

#### Bi-Classification

For the bi-classification task, the performance results **(measured in accuracy)** on the AI-generated text from GPT2, GPT3-re, and LLaMA are summarized in the table below:

|         | SeqXGPT | SeqXGPT (PA) | SeqXGPT (Conv) | SeqXGPT (ps) |
|---------|---------|--------------|----------------|--------------|
| **gpt2**    | 0.65   | 0.72        | 0.72          | 0.70        |
| **gpt3re**  | 0.63   | 0.69        | 0.72          | 0.71        |
| **llama**   | 0.65   | 0.70        | 0.73          | 0.71        |


#### Multi-Classification

For the multi-classification task, the performance results (measured in accuracy) on the AI-generated text from GPT2, GPT3-re, and LLaMA are summarized in the table below:

|         | SeqXGPT | SeqXGPT (PA) | SeqXGPT (Conv) | SeqXGPT (ps) |
|---------|---------|--------------|----------------|--------------|
| **gpt2**    | 0.43   | 0.49        | 0.50          | 0.46        |
| **gpt3re**  | 0.43   | 0.52        | 0.52          | 0.45        |
| **llama**   | 0.44   | 0.52        | 0.51          | 0.49        |
| **human**   | 0.62   | 0.54        | 0.50          | 0.49        |

Overall, these results indicate that our proposed modifications have slightly improve detection performance, enabling robust identification of AI-generated content across different language models. The enhanced feature extraction—through smoothing, convolution, and shuffling—plays a critical role in achieving these improvements.


### Requirements

```bash
cd SeqXGPT
```

```bash
# create a new virtual environment, conda in this case
conda create -n ai-detect python=3.11
conda activate ai-detect
```
```bash
pip install -r requirements.txt
python utils/nltk_download.py
```

```bash
mkdir dataset/SeqXGPT_output # then put every training data (.jsonl files) you want to use here

python SeqXGPT/train.py --split_dataset --data_path dataset/SeqXGPT_output --train_path dataset/SeqXGPT_output/en_gpt2_tr.jsonl --test_path dataset/SeqXGPT_output/en_gpt2_te.jsonl --gpu=0
```

## VI. Application Deployment
Directory `/app` contains all the code for our Streamlit web app. To run the app locally, follow these steps:
```bash
cd app
streamlit run app.py
```
The app is now hosted on Huggingface Streamlit space [here](https://huggingface.co/spaces/Timxjl/seqxgpt_demo).

## VII. Ethical Statement

We acknowledge the responsibility that comes with deploying AI-Detection tools. This technology should be used to support, not supplant, human judgment, and must be applied in a manner that respects individual privacy and free expression.

Key ethical principles include:

- **Responsible Use**: The AI-Text-Detector is designed to combat misinformation and ensure content authenticity. It should not be used for unwarranted surveillance or censorship.
- **Transparency and Oversight**: Automated detection results should be corroborated with human oversight due to the inherent limitations and potential biases in any algorithmic system.
- **Privacy and Legal Compliance**: Users must ensure that the deployment of this technology adheres to all relevant privacy laws and ethical guidelines, safeguarding personal data and consent.
- **Non-Discrimination**: While our model strives for fairness, we remain committed to continually assessing and mitigating any biases present in the detection process.

## Repository structure

```
.
├── Deep_Learning_Model                # Main deep learning implementation directory
│   ├── SeqXGPT                        # Core SeqXGPT model implementation
│   │   ├── Training_Process.txt       # Training process documentation
│   │   ├── dataloader.py              # Data loading utilities
│   │   ├── model.py                   # Model architecture definition
│   │   └── train.py                   # Training script
│   ├── backend_api.py                 # API endpoints for feature extraction
│   ├── backend_model.py               
│   ├── backend_model_info.py          
│   ├── backend_t5.py                  
│   ├── backend_utils.py               
│   ├── dataset                        # Dataset handling
│   │   ├── gen_features.py            # Feature generation script
│   │   └── sample_data                # Sample dataset directory
│   │       ├── inference.jsonl        # Inference data samples
│   │       ├── sample.jsonl           # Training data samples
│   │       └── sample_raw.jsonl       # Raw data samples
│   └── utils                          # Utility scripts
│       ├── api_test.py                # API testing utilities
│       ├── extract_json.py            # JSON data extraction
│       └── nltk_download.py           # NLTK resources downloader
├── Naive_Model                        # Basic model implementation
│   ├── dataset/                       # Dataset for naive model
│   └── naive_approach.py              # Naive model implementation
├── Non_DL                             # Non-deep learning approaches
│   ├── clf.pkl                        # Trained classifier pickle
│   ├── count_vect.pkl                 # Count vectorizer pickle
│   ├── data/                          # Data directory for non-DL models
│   ├── non_dl.py                      # Non-DL model implementation
│   └── tfidf_transformer.pkl          # TF-IDF transformer pickle
└── app                                # Web application directory
    ├── SeqXGPT                        
    │   ├── ...                        # SeqXGPT files
    │   ├── sample_data                # Sample data for app
    │   │   ├── inference_sample.jsonl # Inference samples
    │   │   └── inference_sample_raw.jsonl # Raw inference samples
    │   ├── generate_features.py       # Feature generation for app
    │   ├── transform.py               # Data transformation utilities
    │   └── transformer_cls.pt         # Trained transformer model
    └── app.py                         # Main application file
```

## License

This repository is released under the Apache License 2.0, following the same licensing as the original SeqXGPT project.

--- 
_Some Sections of the README are re-articulated using ChatGPT._
